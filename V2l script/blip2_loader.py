# -*- coding: utf-8 -*-##
#(âœ… RÃ©sultat de blip2_loader.py
#Tu as :ğŸ“¦ ChargÃ© ton fichier image_text_mapping.json contenant +107â€¯000 paires image â†” texte

#ğŸ–¼ï¸ TransformÃ© les images en tenseurs Torch (3Ã—224Ã—224) prÃªts pour BLIP2

#ğŸ§  TokenisÃ© les descriptions agricoles avec Blip2Processor du modÃ¨le Hugging Face

#ğŸ§ª TestÃ© la structure de tes batchs (visuel + langage) avec un batch de 8 exemples

#âœ… ConfirmÃ© que lâ€™environnement Python 3 est bien configurÃ© avec tous les packages nÃ©cessaires

#ğŸ¯ Objectif atteint
#Tu as validÃ© que ton dataset peut nourrir un modÃ¨le BLIP2 â€” avec des images lisibles, des descriptions prÃ©cises, et un format compatible pour lâ€™entraÃ®nement.

#ğŸ› ï¸ Et comme tout a tournÃ© sans erreur, on peut maintenant passer en toute confiance Ã  lâ€™Ã©tape suivante : lâ€™entraÃ®nement du modÃ¨le avec blip2_train.py, puis lâ€™export final vers .keras ou .pt.

#Tu veux que je te lâ€™Ã©crive tout de suite pour lancer le vrai moteurâ€¯? Je peux mÃªme y intÃ©grer un mode debug en --test si tu veux garder le contrÃ´le sur les premiÃ¨res itÃ©rations ğŸ’¥)
import os
import json
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from transformers import Blip2Processor

class BLIP2Dataset(Dataset):
    def __init__(self, mapping_path, processor, image_root, max_samples=None):
        with open(mapping_path, "r", encoding="utf-8") as f:
            self.data = json.load(f)

        self.processor = processor
        self.image_root = image_root
        self.entries = list(self.data.items())

        if max_samples:
            self.entries = self.entries[:max_samples]

        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor()
        ])

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        rel_path, text = self.entries[idx]
        full_path = os.path.join(self.image_root, rel_path)

        image = Image.open(full_path).convert("RGB")
        pixel_values = self.processor.image_processor(image, return_tensors="pt").pixel_values[0]
        inputs = self.processor.tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=128)

        return {
            "pixel_values": pixel_values,
            "input_ids": inputs.input_ids[0],
            "attention_mask": inputs.attention_mask[0],
            "caption": text
        }

# ğŸ”§ Usage
if __name__ == "__main__":
    from transformers import Blip2Processor

    dataset_path = "plantdataset/image_text_mapping.json"
    image_root = "plantdataset"
    processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")

    ds = BLIP2Dataset(dataset_path, processor, image_root, max_samples=1000)
    loader = DataLoader(ds, batch_size=8, shuffle=True)

    for batch in loader:
        print("pixel_values", batch["pixel_values"].shape)
        print("input_ids", batch["input_ids"].shape)
        print("caption", batch["caption"][0])
        break
