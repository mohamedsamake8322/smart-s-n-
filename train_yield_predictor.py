# ğŸ“¦ Import des bibliothÃ¨ques
import pandas as pd
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# ================================
# ğŸ§ª Ã‰TAPE 1 : PrÃ©paration du Dataset
# ================================

# ğŸ“¥ Charger le fichier fusionnÃ©
df = pd.read_csv("dataset_agronomique_final.csv")

# ğŸ”„ Transformer les variables mÃ©tÃ©o verticales en colonnes
df_pivot = df.pivot_table(
    index=["country", "year", "latitude_x", "longitude_x"],
    columns="variable",
    values="value"
).reset_index()

# ğŸ“Œ RÃ©cupÃ©rer les colonnes de sol et intrants agricoles
df_sol = df.drop_duplicates(subset=["latitude_y", "longitude_y"])[[
    "latitude_y", "longitude_y", "ph", "carbon_organic", "nitrogen_total",
    "Value_engrais", "Value_pesticides"
]]

# ğŸ”— Fusion mÃ©tÃ©o + sol
df_merged = pd.merge(
    df_pivot,
    df_sol,
    left_on=["latitude_x", "longitude_x"],
    right_on=["latitude_y", "longitude_y"],
    how="left"
)

# ğŸ“¦ Charger et prÃ©parer la cible FAOSTAT (production)
column_names = [
    "domain_code", "domain", "country", "item_code", "item",
    "element_code", "element", "year_code", "year",
    "unit", "value", "flag"
]
df_fao = pd.read_csv("Production_Crops_Livestock_Afrique.csv")
df_fao = df_fao[df_fao["Element"] == "Production"]
df_fao = df_fao.rename(columns={
    "Area": "country",
    "Year": "year",
    "Item": "culture",
    "Value": "yield_target"
})
print(df_fao.columns.tolist())

# ğŸ”— Fusion finale
df_final = pd.merge(
    df_merged,
    df_fao[["country", "year", "culture", "yield_target"]],
    on=["country", "year"],
    how="left"
)

# ğŸ§¼ Nettoyage
df_final = df_final.dropna(subset=["yield_target"])

# ================================
# ğŸ§  Ã‰TAPE 2 : EntraÃ®nement du modÃ¨le
# ================================

# ğŸ¯ SÃ©lection des variables explicatives
features = [
    "ph", "carbon_organic", "nitrogen_total",
    "Value_engrais", "Value_pesticides",
    "PRECTOTCORR", "WS10M_RANGE", "T2M_MAX",
    "T2M_MIN", "QV2M", "RH2M"
]

# ğŸ” Extraction des donnÃ©es
X = df_final[features]
y = df_final["yield_target"]

# ğŸ“ SÃ©paration en train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# âš¡ï¸ ModÃ¨le XGBoost
model = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    tree_method="hist",  # Compatible avec TPU/GPU
    verbosity=1
)
model.fit(X_train, y_train)

# ğŸ“ˆ Ã‰valuation
y_pred = model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print("\nğŸ“Š Ã‰valuation du modÃ¨le :")
print(f"âœ… RMSE : {rmse:.2f}")
print(f"âœ… RÂ²    : {r2:.2f}")

# ================================
# ğŸ” Ã‰TAPE 3 : Visualisation des importances
# ================================

importances = model.feature_importances_
plt.figure(figsize=(10, 6))
plt.barh(features, importances)
plt.title("ğŸ¯ Importance des variables dans la prÃ©diction de rendement")
plt.xlabel("Importance")
plt.tight_layout()
plt.show()
